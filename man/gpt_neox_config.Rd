% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{gpt_neox_config}
\alias{gpt_neox_config}
\alias{gpt_neox_config_from_pretrained}
\title{Defines a GPTNeoX model configuration}
\usage{
gpt_neox_config(
  vocab_size = 50432,
  hidden_size = 6144,
  num_hidden_layers = 44,
  num_attention_heads = 64,
  intermediate_size = 24576,
  hidden_act = "gelu",
  rotary_pct = 0.25,
  rotary_emb_base = 10000,
  max_position_embeddings = 2048,
  initializer_range = 0.02,
  layer_norm_eps = 1e-05,
  use_cache = TRUE,
  bos_token_id = 0,
  eos_token_id = 2,
  tie_word_embeddings = FALSE,
  use_parallel_residual = TRUE,
  ...
)

gpt_neox_config_from_pretrained(identifier, revision = "main")
}
\arguments{
\item{vocab_size}{(int, optional, defaults to 50432) — Vocabulary size of the GPTNeoX model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling GPTNeoXModel.}

\item{hidden_size}{(int, optional, defaults to 6144) — Dimension of the encoder layers and the pooler layer.}

\item{num_hidden_layers}{(int, optional, defaults to 44) — Number of hidden layers in the Transformer encoder.}

\item{num_attention_heads}{(int, optional, defaults to 64) — Number of attention heads for each attention layer in the Transformer encoder.}

\item{intermediate_size}{(int, optional, defaults to 24576) — Dimension of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.}

\item{hidden_act}{(str or function, optional, defaults to "gelu") — The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu", "selu" and "gelu_new" are supported.}

\item{rotary_pct}{(float, optional, defaults to 0.25) — percentage of hidden dimensions to allocate to rotary embeddings}

\item{rotary_emb_base}{(int, optional, defaults to 10000) — base for computing rotary embeddings frequency}

\item{max_position_embeddings}{(int, optional, defaults to 2048) — The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).}

\item{initializer_range}{(float, optional, defaults to 1e-5) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.}

\item{layer_norm_eps}{(float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.}

\item{use_cache}{(bool, optional, defaults to True) — Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if config.is_decoder=True.}

\item{use_parallel_residual}{(bool, optional, defaults to True) — Whether to use a “parallel” formulation in each Transformer layer, which can provide a slight training speedup at large scales (e.g. 20B). Example —}

\item{...}{Additional configuration options.}
}
\description{
Defines a GPTNeoX model configuration
}
\section{Functions}{
\itemize{
\item \code{gpt_neox_config()}: Defines the configuration of the tokenizer.

\item \code{gpt_neox_config_from_pretrained()}: Uses a configuration defined a HF hub repository.

}}
